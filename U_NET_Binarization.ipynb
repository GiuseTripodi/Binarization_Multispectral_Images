{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCXcUu9f5kip",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Image Binarization\n",
    "\n",
    "The dataset consist of historical images, the class are:\n",
    "- Class 1: background\n",
    "- Class 2: foreground\n",
    "\n",
    "This notebook is the exactly google colab notebook I have used to train and test the model. All the images are taken from my google drive. The images used to test the approaches are in the directory \"Test_images_approach_\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIGdTlJ06ZHD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Dataset and Basic Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKzfBvN7vot0",
    "outputId": "293f2946-e7d1-450e-f048-1f0496f7ac0f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[1;32m      2\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0027KhNdEK3u",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# code from: https://colab.research.google.com/github/VidushiBhatia/U-Net-Implementation/blob/main/U_Net_for_Image_Segmentation_From_Scratch_Using_TensorFlow_v4.ipynb#scrollTo=dwjr7eZQEK36\n",
    "\n",
    "# for data load\n",
    "import os\n",
    "\n",
    "# for reading and processing images\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np # for using np arrays\n",
    "\n",
    "# for bulding and running deep learning model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks, metrics\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = 'U-Net'\n",
    "os.makedirs(f'/content/drive/MyDrive/models/{MODEL_NAME}', exist_ok = True)\n",
    "N_CLASSES = 2\n",
    "\n",
    "MC_BEST = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/MyDrive/models/{MODEL_NAME}/best-{MODEL_NAME}.hdf5', save_best_only=True, save_weights_only=True, mode='max', save_freq=\"epoch\", verbose=1)\n",
    "LOG = callbacks.CSVLogger(f'/content/drive/MyDrive/models/{MODEL_NAME}/{MODEL_NAME}.log', append=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPnUttk2SpR-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Helper Function for Data Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1  Load  and Process Data"
   ],
   "metadata": {
    "id": "fwNi4bFOE-BQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nziMCqauEK3x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# code from: https://colab.research.google.com/github/VidushiBhatia/U-Net-Implementation/blob/main/U_Net_for_Image_Segmentation_From_Scratch_Using_TensorFlow_v4.ipynb#scrollTo=dwjr7eZQEK36\n",
    "\n",
    "def LoadData (path1, path2):\n",
    "    \"\"\"\n",
    "    Looks for relevant filenames in the shared path\n",
    "    Returns 2 lists for original and masked files respectively\n",
    "    \n",
    "    \"\"\"\n",
    "    # Read the images folder like a list\n",
    "    image_dataset = os.listdir(path1)\n",
    "    mask_dataset = os.listdir(path2)\n",
    "\n",
    "    # Make a list for images and masks filenames\n",
    "    orig_img = []\n",
    "    mask_img = []\n",
    "    for file in image_dataset:\n",
    "        orig_img.append(file)\n",
    "    for file in mask_dataset:\n",
    "        mask_img.append(file)\n",
    "\n",
    "    # Sort the lists to get both of them in same order (the dataset has exactly the same name for images and corresponding masks)\n",
    "    orig_img.sort()\n",
    "    mask_img.sort()\n",
    "    \n",
    "    return orig_img, mask_img\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrJBCoMKThYO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_image_to_tile_multiple(image3, tile_size, padding=\"CONSTANT\"):\n",
    "    imagesize = image3.shape\n",
    "    if (imagesize[0] % tile_size[0]) != 0 or (imagesize[1] % tile_size[1]) != 0: #it is not of the correct size \n",
    "  \n",
    "      target_height = imagesize[0] - (imagesize[0] % tile_size[0]) + tile_size[0]\n",
    "      target_width = imagesize[1] - (imagesize[1] % tile_size[1]) + tile_size[1]\n",
    "\n",
    "      add_height = target_height - imagesize[0]\n",
    "      add_width = target_width - imagesize[1]\n",
    "\n",
    "      ret = np.pad(image3, ((0, add_height) , (0, add_width)), \"constant\", constant_values=0)\n",
    "    else: #if it is already of the correct size \n",
    "      ret = image3\n",
    "\n",
    "    return ret\n",
    "\n",
    "def reverse_padding(original_image, changed_image):\n",
    "  original_size = original_image.shape\n",
    "  return changed_image[0:original_size[0], 0:original_size[1]]\n",
    "    \n",
    "# code adapted from: https://stackoverflow.com/questions/38235643/getting-started-with-tensorflow-split-image-into-sub-images\n",
    "\n",
    "def split_image(image3, tile_size):\n",
    "    image_shape = tf.shape(image3)\n",
    "    tile_rows = tf.reshape(image3, [image_shape[0], -1, tile_size[1]])\n",
    "    serial_tiles = tf.transpose(tile_rows, [1, 0, 2])\n",
    "    return tf.reshape(serial_tiles, [-1, tile_size[1], tile_size[0]])\n",
    "\n",
    "def unsplit_image(tiles4, image_shape):\n",
    "    tile_width = tf.shape(tiles4)[1]\n",
    "    serialized_tiles = tf.reshape(tiles4, [-1, image_shape[0], tile_width])\n",
    "    rowwise_tiles = tf.transpose(serialized_tiles, [1, 0, 2])\n",
    "    return tf.reshape(rowwise_tiles, [image_shape[0], image_shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray"
   ],
   "metadata": {
    "id": "v6PGdfbfgWnV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3Y0ZdqCOoAb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function get the images and the training mask, and fro every every image, mask create\n",
    "a block of size [target_shape_img], and add this block to the training array X\n",
    "\n",
    "'''\n",
    "def PreprocessData(img, mask, target_shape_block, path1, path2, change_color):\n",
    "  '''\n",
    "  Processes the images and mask present in the shared list and path, split the images\n",
    "  and the masks in the target_shape_block indicated, and add the block to the output list\n",
    "\n",
    "  target_shape_block is a 2 dimensional array, with the expected blok size\n",
    "  '''\n",
    "\n",
    "  X = {}\n",
    "  y = {}\n",
    "  shape_images_dict = {}\n",
    "  for file in img:\n",
    "    index = img.index(file)\n",
    "    single_mask_ind = mask[index]\n",
    "\n",
    "    #get images and mask\n",
    "    single_image = imageio.imread(os.path.join(path1, file))\n",
    "    if change_color:\n",
    "      single_image = cv2.cvtColor(single_image, cv2.COLOR_BGR2GRAY)\n",
    "    single_image = single_image / 255\n",
    "\n",
    "    single_mask = imageio.imread(os.path.join(path2, single_mask_ind))\n",
    "    if change_color:\n",
    "      single_mask = cv2.cvtColor(single_mask, cv2.COLOR_BGR2GRAY)\n",
    "    single_mask = single_mask / 255\n",
    "\n",
    "    #split the images, both images and mash have the same size\n",
    "    \n",
    "    single_image = pad_image_to_tile_multiple(single_image, target_shape_block)\n",
    "    single_mask = pad_image_to_tile_multiple(single_mask, target_shape_block)\n",
    "\n",
    "    single_image=tf.convert_to_tensor(single_image,dtype=tf.float32)# this is converting the the numpy array further\n",
    "    single_mask = tf.convert_to_tensor(single_mask, dtype=tf.float32)\n",
    "\n",
    "    tiles_image = split_image(single_image, target_shape_block)\n",
    "    tiles_mask = split_image(single_mask, target_shape_block)\n",
    "\n",
    "    shape_images_dict[file] = single_image.shape\n",
    "\n",
    "    for tile in range(len(tiles_image)):\n",
    "      X[file.replace(\".png\", f\"_{tile}\")] = tiles_image[tile]\n",
    "      y[file.replace(\".png\", f\"_{tile}\")] = tiles_mask[tile]\n",
    "\n",
    "  #create a proper dataset\n",
    "  m = len(X.keys())\n",
    "  i_h = target_shape_block[0]\n",
    "  i_w = target_shape_block[1]\n",
    "  X_ = np.zeros((m, i_h, i_w), dtype=np.float)\n",
    "  y_ = np.zeros((m, i_h, i_w), dtype=np.int32)\n",
    "\n",
    "  for i in range(m):\n",
    "    X_[i] = X[list(X.keys())[i]]\n",
    "    y_[i] = y[list(y.keys())[i]]\n",
    "\n",
    "  return X_, y_, shape_images_dict #return also the size of the images after the padding"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Results of Validation Dataset\n",
    "def VisualizeResults(index):\n",
    "    img = X_valid[index]\n",
    "    img = img[np.newaxis, ...]\n",
    "    pred_y = unet.predict(img)\n",
    "    pred_mask = tf.argmax(pred_y[0], axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    fig, arr = plt.subplots(1, 3, figsize=(15, 15))\n",
    "    arr[0].imshow(X_valid[index])\n",
    "    arr[0].set_title('Processed Image')\n",
    "    arr[1].imshow(y_valid[index])\n",
    "    arr[1].set_title('Actual Masked Image ')\n",
    "    arr[2].imshow(pred_mask[:,:,0])\n",
    "    arr[2].set_title('Predicted Masked Image ')\n",
    "    "
   ],
   "metadata": {
    "id": "n8uH9WyuEe57",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTiBe36xRzoQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##1.2 Constructing the U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgnsdxWlEK32",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# code from: https://colab.research.google.com/github/VidushiBhatia/U-Net-Implementation/blob/main/U_Net_for_Image_Segmentation_From_Scratch_Using_TensorFlow_v4.ipynb#scrollTo=2hCcs14OEK4C\n",
    "\n",
    "\n",
    "#U-Net Encoder Block\n",
    "def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):\n",
    "    \"\"\"\n",
    "    This block uses multiple convolution layers, max pool, relu activation to create an architecture for learning. \n",
    "    Dropout can be added for regularization to prevent overfitting. \n",
    "    The block returns the activation values for next layer along with a skip connection which will be used in the decoder\n",
    "    \"\"\"\n",
    "    # Add 2 Conv Layers with relu activation and HeNormal initialization using TensorFlow \n",
    "    # Proper initialization prevents from the problem of exploding and vanishing gradients \n",
    "    # 'Same' padding will pad the input to conv layer such that the output has the same height and width (hence, is not reduced in size) \n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,   # Kernel size   \n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(inputs)\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,   # Kernel size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(conv)\n",
    "    \n",
    "    # Batch Normalization will normalize the output of the last layer based on the batch's mean and standard deviation\n",
    "    conv = BatchNormalization()(conv, training=False)\n",
    "\n",
    "    # In case of overfitting, dropout will regularize the loss and gradient computation to shrink the influence of weights on output\n",
    "    if dropout_prob > 0:     \n",
    "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "\n",
    "    # Pooling reduces the size of the image while keeping the number of channels same\n",
    "    # Pooling has been kept as optional as the last encoder layer does not use pooling (hence, makes the encoder block flexible to use)\n",
    "    # Below, Max pooling considers the maximum of the input slice for output computation and uses stride of 2 to traverse across input image\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "    else:\n",
    "        next_layer = conv\n",
    "\n",
    "    # skip connection (without max pooling) will be input to the decoder layer to prevent information loss during transpose convolutions      \n",
    "    skip_connection = conv\n",
    "    \n",
    "    return next_layer, skip_connection\n",
    "\n",
    "# U-Net Decoder Block\n",
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
    "    \"\"\"\n",
    "    Decoder Block first uses transpose convolution to upscale the image to a bigger size and then,\n",
    "    merges the result with skip layer results from encoder block\n",
    "    Adding 2 convolutions with 'same' padding helps further increase the depth of the network for better predictions\n",
    "    The function returns the decoded layer output\n",
    "    \"\"\"\n",
    "    # Start with a transpose convolution layer to first increase the size of the image\n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,\n",
    "                 (3,3),    # Kernel size\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(prev_layer_input)\n",
    "\n",
    "    # Merge the skip connection from previous block to prevent information loss\n",
    "    merge = concatenate([up, skip_layer_input], axis=3)\n",
    "    \n",
    "    # Add 2 Conv Layers with relu activation and HeNormal initialization for further processing\n",
    "    # The parameters for the function are similar to encoder\n",
    "    conv = Conv2D(n_filters, \n",
    "                 3,     # Kernel size\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(merge)\n",
    "    conv = Conv2D(n_filters,\n",
    "                 3,   # Kernel size\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(conv)\n",
    "    return conv\n",
    "\n",
    "# Compile U-Net Blocks\n",
    "def UNetCompiled(input_size=(128, 128, 3), n_filters=32, n_classes=3):\n",
    "    \"\"\"\n",
    "    Combine both encoder and decoder blocks according to the U-Net research paper\n",
    "    Return the model as output \n",
    "    \"\"\"\n",
    "    # Input size represent the size of 1 image (the size used for pre-processing) \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
    "    # Observe that the filters are increasing as we go deeper into the network which will increasse the # channels of the image \n",
    "    cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
    "    cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
    "    cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
    "    cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=0.3, max_pooling=True)\n",
    "    cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=0.3, max_pooling=False) \n",
    "    \n",
    "    # Decoder includes multiple mini blocks with decreasing number of filters\n",
    "    # Observe the skip connections from the encoder are given as input to the decoder\n",
    "    # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
    "    ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "    ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
    "    ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
    "    ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "    # Complete the model with 1 3x3 convolution layer (Same as the prev Conv Layers)\n",
    "    # Followed by a 1x1 Conv layer to get the image to the desired size. \n",
    "    # Observe the number of channels will be equal to number of output classes\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                 3,\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, 1, padding='same')(conv9)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Approach 1: Concatenation Features"
   ],
   "metadata": {
    "id": "EENdbWedCuUj",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Load and View Data"
   ],
   "metadata": {
    "id": "bD-CzaoaC2_0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Load Train Set and view some examples \"\"\"\n",
    "# Call the apt function\n",
    "path1 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_1/org/'\n",
    "path2 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_1/gt/'\n",
    "img, mask = LoadData (path1, path2)\n",
    "\n",
    "# View an example of image and corresponding mask \n",
    "show_images = 4\n",
    "for i in range(show_images):\n",
    "    img_view  = imageio.imread(path1 + img[i])\n",
    "    img_view = cv2.cvtColor(img_view, cv2.COLOR_BGR2GRAY)\n",
    "    mask_view = imageio.imread(path2 + mask[i])\n",
    "    mask_view = cv2.cvtColor(mask_view, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    print(img_view.shape)\n",
    "    print(mask_view.shape)\n",
    "    fig, arr = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    arr[0].imshow(img_view)\n",
    "    arr[0].set_title('Image '+ str(i))\n",
    "    arr[1].imshow(mask_view)\n",
    "    arr[1].set_title('Masked Image '+ str(i))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "wPIZ4Bo5CyXF",
    "outputId": "133b7451-e0d0-466a-e026-ff2e5437434b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Preprocessing Data"
   ],
   "metadata": {
    "id": "H7si7F0RDAnU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the desired shape\n",
    "target_shape_img_blok = [128, 128]\n",
    "\n",
    "# Process data using apt helper function\n",
    "X, y, _ = PreprocessData(img, mask, target_shape_img_blok, path1, path2, True)\n",
    "\n",
    "# QC the shape of output and classes in output dataset \n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"Y shape:\", y.shape)\n",
    "# There are 3 classes : background, pet, outline\n",
    "print(np.unique(y))\n",
    "\n",
    "# Visualize the output\n",
    "image_index = 2\n",
    "fig, arr = plt.subplots(1, 2, figsize=(15, 15))\n",
    "arr[0].imshow(X[image_index])\n",
    "arr[0].set_title('Processed Image')\n",
    "arr[1].imshow(y[image_index])\n",
    "arr[1].set_title('Processed Masked Image ')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "MmPk1z1LDCdE",
    "outputId": "d878a26c-19a1-4586-ee79-facc99868900",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ],
   "metadata": {
    "id": "8lY55gyInUJZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Build the mdoel\n",
    "\n"
   ],
   "metadata": {
    "id": "8D_XMz7NDIX_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Call the helper function for defining the layers for the model, given the input image size\n",
    "unet = UNetCompiled(input_size=(128,128,1), n_filters=32, n_classes=2)\n",
    "\n",
    "# Check the summary to better interpret how the output dimensions change in each layer\n",
    "#unet.summary()"
   ],
   "metadata": {
    "id": "QtGE4be5DO52",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Compile and Run the model"
   ],
   "metadata": {
    "id": "dap7NKI1DRDg",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "unet.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "id": "7n3PzXD1Dih6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the model in a mini-batch fashion and compute the progress for each epoch\n",
    "batch_size= 32\n",
    "epochs = 15\n",
    "results = unet.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=[MC_BEST, LOG])\n",
    "hist =results.history"
   ],
   "metadata": {
    "id": "LKkO8hJFD7bF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cf275a70-7e9c-462c-a919-b9d0e1d7e524",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 Evaluate Model Result"
   ],
   "metadata": {
    "id": "v8JLL_LnEAeN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "unet = UNetCompiled(input_size=(128,128,1), n_filters=32, n_classes=2)\n",
    "unet.load_weights(f'/content/drive/MyDrive/models/{MODEL_NAME}/best-{MODEL_NAME}.hdf5')\n",
    "unet.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "unet.load_weights(f'/content/drive/MyDrive/models/{MODEL_NAME}/best-{MODEL_NAME}.hdf5')\n",
    "\n",
    "#load history\n",
    "hist = pd.read_csv(f'/content/drive/MyDrive/models/{MODEL_NAME}/{MODEL_NAME}.log', sep=\",\", engine=\"python\")\n",
    "'''"
   ],
   "metadata": {
    "id": "Zzcqj6Q1EDJS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# High Bias is a characteristic of an underfitted model and we would observe low accuracies for both train and validation set\n",
    "# High Variance is a characterisitic of an overfitted model and we would observe high accuracy for train set and low for validation set\n",
    "# To check for bias and variance plit the graphs for accuracy \n",
    "# I have plotted for loss too, this helps in confirming if the loss is decreasing with each iteration - hence, the model is optimizing fine\n",
    "fig, axis = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axis[0].plot(hist[\"loss\"], color='r', label = 'train loss')\n",
    "axis[0].plot(hist[\"val_loss\"], color='b', label = 'dev loss')\n",
    "axis[0].set_title('Loss Comparison')\n",
    "axis[0].legend()\n",
    "axis[1].plot(hist[\"accuracy\"], color='r', label = 'train accuracy')\n",
    "axis[1].plot(hist[\"val_accuracy\"], color='b', label = 'dev accuracy')\n",
    "axis[1].set_title('Accuracy Comparison')\n",
    "axis[1].legend()\n",
    "\n",
    "# RESULTS\n",
    "# The train loss is consistently decreasing showing that Adam is able to optimize the model and find the minima\n",
    "# The accuracy of train and validation is ~90% which is high enough, so low bias\n",
    "# and the %s aren't that far apart, hence low variance"
   ],
   "metadata": {
    "id": "oDkwwKpPEKGS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6 View Predicted Segmentations and Test on validation images"
   ],
   "metadata": {
    "id": "zqZg4Np6EOhT",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "unet.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "id": "6K_PKkLbEUKL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Add any index to contrast the predicted mask with actual mask\n",
    "index = 8\n",
    "VisualizeResults(index)"
   ],
   "metadata": {
    "id": "Ht9hGSqKEZno",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test on different Images"
   ],
   "metadata": {
    "id": "hHoNCJmjEnWN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "path1 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_1/val_images/org/'\n",
    "path2 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_1/val_images/gt/'\n",
    "img, mask = LoadData (path1, path2)\n",
    "\n",
    "# Define the desired shape\n",
    "target_shape_img_blok = [128, 128]\n",
    "\n",
    "# Process data using apt helper function\n",
    "X, y, shape_images_dict = PreprocessData(img, mask, target_shape_img_blok, path1, path2, False)\n",
    "\n",
    "# QC the shape of output and classes in output dataset \n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"Y shape:\", y.shape)\n",
    "# There are 3 classes : background, pet, outline\n",
    "print(np.unique(y))\n",
    "\n",
    "# Visualize the output\n",
    "image_index = 9\n",
    "fig, arr = plt.subplots(1, 2, figsize=(15, 15))\n",
    "arr[0].imshow(X[image_index])\n",
    "arr[0].set_title('Processed Image')\n",
    "arr[1].imshow(y[image_index])\n",
    "arr[1].set_title('Processed Masked Image ')"
   ],
   "metadata": {
    "id": "tp3NrI-REo7R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# doing the prediction\n",
    "img = X[0]\n",
    "img = img[np.newaxis, ...]\n",
    "pred_y = unet.predict(img)\n",
    "pred_mask_test = tf.argmax(pred_y[0], axis=-1)\n",
    "pred_mask_test = pred_mask_test[..., tf.newaxis]\n",
    "\n",
    "first = True\n",
    "#for each box do the prediction and get the results\n",
    "for i in range(1,X.shape[0]):\n",
    "# doing the prediction\n",
    "  img = X[i]\n",
    "  img = img[np.newaxis, ...]\n",
    "  pred_y = unet.predict(img)\n",
    "  pred_mask = tf.argmax(pred_y[0], axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  \n",
    "  if first:\n",
    "    blocks = tf.stack((pred_mask_test[:,:,0], pred_mask[:,:,0]), axis=0)\n",
    "    gt_blocks = tf.stack((y[0], y[i]), axis=0)\n",
    "    first = False\n",
    "  else:\n",
    "    add = tf.concat(pred_mask[:,:,0], axis=1)\n",
    "    add = tf.reshape(add, [1,target_shape_img_blok[0], target_shape_img_blok[1]])\n",
    "    blocks = tf.concat((blocks, add), axis=0)\n",
    "\n",
    "    add = tf.concat(y[i], axis=1)\n",
    "    add = tf.reshape(add, [1,target_shape_img_blok[0], target_shape_img_blok[1]])\n",
    "    gt_blocks = tf.concat((gt_blocks, add), axis=0)"
   ],
   "metadata": {
    "id": "h3k7QK_QEiWP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "shape_images_dict.keys()"
   ],
   "metadata": {
    "id": "nYBmDsTuZ32V",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#create the images\n",
    "shape_block = target_shape_img_blok\n",
    "image_shape_pad = [shape_images_dict[\"z802.png\"], shape_images_dict[\"z822.png\"]]\n",
    "Test_images_approach_1 = \"/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_1\"\n",
    "\n",
    "#create a directory with the name of the validation images\n",
    "dir_val_img = [\"z802\", \"z822\"]\n",
    "file_name = \"F1s.png\" #name of the file im going to take to reverse the passing\n",
    "\n",
    "\n",
    "b = 0\n",
    "for i in range(2): #for each validation images\n",
    "  img_shape = image_shape_pad[i]\n",
    "  n_blocks_for_image = round((img_shape[0] * img_shape[1]) / (shape_block[0] * shape_block[1]))\n",
    "  \n",
    "\n",
    "  # reconstruct the predicted images, and save thos in the apposit directory\n",
    "  tiles = tf.slice(blocks, begin=[b, 0, 0], size=[n_blocks_for_image, shape_block[0], shape_block[1]])\n",
    "  img = unsplit_image(tiles, img_shape)\n",
    "\n",
    "  #Crete the GT image\n",
    "  gt_tiles = tf.slice(gt_blocks, begin=[b, 0, 0], size=[n_blocks_for_image, shape_block[0], shape_block[1]])\n",
    "  gt_img = unsplit_image(gt_tiles, img_shape)\n",
    "\n",
    "  b += n_blocks_for_image\n",
    "\n",
    "  #remove the padding before saving the images\n",
    "\n",
    "  #get the image without padding\n",
    "  img_no_pad = imageio.imread(os.path.join(Test_images_approach_1, \"val_images\", dir_val_img[i], file_name))\n",
    "  img = reverse_padding(img_no_pad, img)\n",
    "  gt_img = reverse_padding(img_no_pad, gt_img)\n",
    "\n",
    "  #save the image\n",
    "  plt.imsave(f\"{Test_images_approach_1}/output/gt/FN{i}_GT.png\", gt_img)\n",
    "  plt.imsave(f\"{Test_images_approach_1}/output/images/FN{i}.png\", img)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "qWKFrRmcEurX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqLkCAtHSVOZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Approach 2: Executing above Functions to Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 3.1 Load and View Data"
   ],
   "metadata": {
    "id": "-GWywIe_DEtm",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwjr7eZQEK36",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Load Train Set and view some examples \"\"\"\n",
    "# Call the apt function\n",
    "path1 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_2/org/'\n",
    "path2 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_2/gt/'\n",
    "img, mask = LoadData (path1, path2)\n",
    "\n",
    "# View an example of image and corresponding mask \n",
    "show_images = 5\n",
    "for i in range(show_images):\n",
    "    img_view  = imageio.imread(path1 + img[i])\n",
    "    mask_view = imageio.imread(path2 + mask[i])\n",
    "    print(img_view.shape)\n",
    "    print(mask_view.shape)\n",
    "    fig, arr = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    arr[0].imshow(img_view)\n",
    "    arr[0].set_title('Image '+ str(i))\n",
    "    arr[1].imshow(mask_view)\n",
    "    arr[1].set_title('Masked Image '+ str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwmeaNKfSjrf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJG-f4ccxqkf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the desired shape\n",
    "target_shape_img_blok = [128, 128]\n",
    "\n",
    "# Process data using apt helper function\n",
    "X, y, image_shape_pad = PreprocessData(img, mask, target_shape_img_blok, path1, path2, False)\n",
    "\n",
    "# QC the shape of output and classes in output dataset \n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"Y shape:\", y.shape)\n",
    "# There are 3 classes : background, pet, outline\n",
    "print(np.unique(y))\n",
    "\n",
    "# Visualize the output\n",
    "image_index = 0\n",
    "fig, arr = plt.subplots(1, 2, figsize=(15, 15))\n",
    "arr[0].imshow(X[image_index])\n",
    "arr[0].set_title('Processed Image')\n",
    "arr[1].imshow(y[image_index])\n",
    "arr[1].set_title('Processed Masked Image ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "At5pDPYpyBxy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Use scikit-learn's function to split the dataset\n",
    "# Here, I have used 20% data as test/valid set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pry8G0SXXhv3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3 Buld U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hCcs14OEK4C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Call the helper function for defining the layers for the model, given the input image size\n",
    "unet = UNetCompiled(input_size=(128,128,1), n_filters=32, n_classes=2)\n",
    "\n",
    "# Check the summary to better interpret how the output dimensions change in each layer\n",
    "#unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypAjQ7SMX24S",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.4 Compile and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2aOnXtAX7xd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unet.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHeSFYhOEK4F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run the model in a mini-batch fashion and compute the progress for each epoch\n",
    "batch_size= 32\n",
    "epochs = 10\n",
    "results = unet.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=[MC_BEST, LOG])\n",
    "hist =results.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A-UihBftiE1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.5 Evaluate Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxHtcQftZhum",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "unet = UNetCompiled(input_size=(128,128,1), n_filters=32, n_classes=2)\n",
    "unet.load_weights(f'/content/drive/MyDrive/models/{MODEL_NAME}/best-{MODEL_NAME}.hdf5')\n",
    "unet.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "unet.load_weights(f'/content/drive/MyDrive/models/{MODEL_NAME}/best-{MODEL_NAME}.hdf5')\n",
    "\n",
    "#load history\n",
    "hist = pd.read_csv(f'/content/drive/MyDrive/models/{MODEL_NAME}/{MODEL_NAME}.log', sep=\",\", engine=\"python\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48eePS01nQsT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# High Bias is a characteristic of an underfitted model and we would observe low accuracies for both train and validation set\n",
    "# High Variance is a characterisitic of an overfitted model and we would observe high accuracy for train set and low for validation set\n",
    "# To check for bias and variance plit the graphs for accuracy \n",
    "# I have plotted for loss too, this helps in confirming if the loss is decreasing with each iteration - hence, the model is optimizing fine\n",
    "fig, axis = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axis[0].plot(hist[\"loss\"], color='r', label = 'train loss')\n",
    "axis[0].plot(hist[\"val_loss\"], color='b', label = 'dev loss')\n",
    "axis[0].set_title('Loss Comparison')\n",
    "axis[0].legend()\n",
    "axis[1].plot(hist[\"accuracy\"], color='r', label = 'train accuracy')\n",
    "axis[1].plot(hist[\"val_accuracy\"], color='b', label = 'dev accuracy')\n",
    "axis[1].set_title('Accuracy Comparison')\n",
    "axis[1].legend()\n",
    "\n",
    "# RESULTS\n",
    "# The train loss is consistently decreasing showing that Adam is able to optimize the model and find the minima\n",
    "# The accuracy of train and validation is ~90% which is high enough, so low bias\n",
    "# and the %s aren't that far apart, hence low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KvfTuURhRK_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.6 View Predicted Segmentations and Test on validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6SAZrsye1ak",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unet.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Add any index to contrast the predicted mask with actual mask\n",
    "index = 7\n",
    "VisualizeResults(index)"
   ],
   "metadata": {
    "id": "kS7cIrktCD42",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test on binarized different images"
   ],
   "metadata": {
    "id": "VSk38SE1RbfX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "path1 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_2/val_images/org/'\n",
    "path2 = '/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_2/val_images/gt/'\n",
    "img, mask = LoadData (path1, path2)\n",
    "\n",
    "# Define the desired shape\n",
    "target_shape_img_blok = [128, 128]\n",
    "\n",
    "# Process data using apt helper function\n",
    "X, y, shape_images_dict = PreprocessData(img, mask, target_shape_img_blok, path1, path2, False)\n",
    "\n",
    "# QC the shape of output and classes in output dataset \n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"Y shape:\", y.shape)\n",
    "# There are 3 classes : background, pet, outline\n",
    "print(np.unique(y))\n",
    "\n",
    "# Visualize the output\n",
    "image_index = 9\n",
    "fig, arr = plt.subplots(1, 2, figsize=(15, 15))\n",
    "arr[0].imshow(X[image_index])\n",
    "arr[0].set_title('Processed Image')\n",
    "arr[1].imshow(y[image_index])\n",
    "arr[1].set_title('Processed Masked Image ')\n"
   ],
   "metadata": {
    "id": "J7KMbGRJRhTY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# doing the prediction\n",
    "img = X[0]\n",
    "img = img[np.newaxis, ...]\n",
    "pred_y = unet.predict(img)\n",
    "pred_mask_test = tf.argmax(pred_y[0], axis=-1)\n",
    "pred_mask_test = pred_mask_test[..., tf.newaxis]\n",
    "\n",
    "first = True\n",
    "#for each box do the prediction and get the results\n",
    "for i in range(1,X.shape[0]):\n",
    "# doing the prediction\n",
    "  img = X[i]\n",
    "  img = img[np.newaxis, ...]\n",
    "  pred_y = unet.predict(img)\n",
    "  pred_mask = tf.argmax(pred_y[0], axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  \n",
    "  if first:\n",
    "    blocks = tf.stack((pred_mask_test[:,:,0], pred_mask[:,:,0]), axis=0)\n",
    "    gt_blocks = tf.stack((y[0], y[i]), axis=0)\n",
    "    first = False\n",
    "  else:\n",
    "    add = tf.concat(pred_mask[:,:,0], axis=1)\n",
    "    add = tf.reshape(add, [1,target_shape_img_blok[0], target_shape_img_blok[1]])\n",
    "    blocks = tf.concat((blocks, add), axis=0)\n",
    "\n",
    "    add = tf.concat(y[i], axis=1)\n",
    "    add = tf.reshape(add, [1,target_shape_img_blok[0], target_shape_img_blok[1]])\n",
    "    gt_blocks = tf.concat((gt_blocks, add), axis=0)"
   ],
   "metadata": {
    "id": "sqetqU08lQgV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "shape_images_dict.keys()"
   ],
   "metadata": {
    "id": "oOfyPxr6W1X-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create the images\n",
    "shape_block = target_shape_img_blok\n",
    "image_shape_pad = [shape_images_dict[\"z802.png\"], shape_images_dict[\"z822.png\"]]\n",
    "Test_images_approach_2 = \"/content/drive/MyDrive/binarization_data/model_images/Test_images_approach_2\"\n",
    "\n",
    "#create a directory with the name of the validation images\n",
    "dir_val_img = [\"z802\", \"z822\"]\n",
    "file_name = \"F1s.png\" #name of the file im going to take to reverse the passing\n",
    "\n",
    "\n",
    "b = 0\n",
    "for i in range(2): #for each validation images\n",
    "  img_shape = image_shape_pad[i]\n",
    "  n_blocks_for_image = round((img_shape[0] * img_shape[1]) / (shape_block[0] * shape_block[1]))\n",
    "  \n",
    "\n",
    "  # reconstruct the predicted images, and save thos in the apposit directory\n",
    "  tiles = tf.slice(blocks, begin=[b, 0, 0], size=[n_blocks_for_image, shape_block[0], shape_block[1]])\n",
    "  img = unsplit_image(tiles, img_shape)\n",
    "  \n",
    "\n",
    "  #Crete the GT image\n",
    "  gt_tiles = tf.slice(gt_blocks, begin=[b, 0, 0], size=[n_blocks_for_image, shape_block[0], shape_block[1]])\n",
    "  gt_img = unsplit_image(gt_tiles, img_shape)\n",
    "\n",
    "  b += n_blocks_for_image\n",
    "\n",
    "\n",
    "  #remove the padding before saving the images\n",
    "\n",
    "  #get the image without padding\n",
    "  img_no_pad = imageio.imread(os.path.join(Test_images_approach_2, \"val_images\", dir_val_img[i], file_name))\n",
    "  img = reverse_padding(img_no_pad, img)\n",
    "  gt_img = reverse_padding(img_no_pad, gt_img)\n",
    "\n",
    "  #save the image\n",
    "  plt.imsave(f\"{Test_images_approach_2}/output/gt/FN{i}_GT.png\", gt_img)\n",
    "  plt.imsave(f\"{Test_images_approach_2}/output/images/FN{i}.png\", img)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "U-NET-Binarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}